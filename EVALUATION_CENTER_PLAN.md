# EVALUATION CENTER: Real-Time Monitoring & Testing Suite

**Status**: Critical Missing Features - Evaluation runs blind with no visibility
**Problem**: User can't see what the agent is doing, no conversation, no step details, no comparison tools

---

## ğŸš¨ CRITICAL ISSUES (What's Broken Now)

### 1. **No Real-Time Agent Visibility**
- âŒ Can't see agent's thinking/conversation
- âŒ Can't see tool calls (Read/Edit/Bash) with outputs
- âŒ Can't see files being modified live
- âŒ Can't see error messages from tools
- âŒ Can't see test outputs when they run

### 2. **No Problem/Answer Context**
- âŒ Problem statement not shown during eval
- âŒ Expected solution not visible
- âŒ Agent's actual solution not captured
- âŒ Can't compare "what it should do" vs "what it did"
- âŒ No way to see WHY a task failed

### 3. **No Historical Data**
- âŒ Can't view past evaluation runs
- âŒ Can't compare new run with old runs
- âŒ No persistent storage of detailed results
- âŒ No trend analysis (is M2 better than M1 over time?)

### 4. **No Interactive Testing**
- âŒ Must start a new eval to see anything
- âŒ Can't browse old results
- âŒ Can't re-run single failed tasks
- âŒ Can't drill down into specific task execution

---

## ğŸ“Š P0 FEATURES (Must Have - Core Functionality)

### P0.1: **Real-Time Agent Conversation View**
**What**: Show the actual LLM conversation as it happens

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ¤– Agent Conversation - Task django__django-12345       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Step 1/25  Tool: None  Tokens: 0  Cost: $0.00           â”‚
â”‚                                                           â”‚
â”‚ ğŸ§  Assistant [thinking...]                              â”‚
â”‚ I need to understand the problem first. The issue is    â”‚
â”‚ about migration commands not handling dependencies      â”‚
â”‚ correctly. Let me start by reading the migration file.  â”‚
â”‚                                                           â”‚
â”‚ Step 2/25  Tool: Read  Tokens: 1,234  Cost: $0.02      â”‚
â”‚                                                           â”‚
â”‚ ğŸ”§ Tool Call: Read                                       â”‚
â”‚ File: src/django/core/management/commands/migrate.py    â”‚
â”‚                                                           â”‚
â”‚ ğŸ“„ Tool Output:                                          â”‚
â”‚ ```python                                                 â”‚
â”‚ class Command(BaseCommand):                              â”‚
â”‚     def handle(self, *args, **options):                  â”‚
â”‚         # Migration logic here...                        â”‚
â”‚ ```                                                       â”‚
â”‚                                                           â”‚
â”‚ Step 3/25  Tool: None  Tokens: 2,456  Cost: $0.04      â”‚
â”‚                                                           â”‚
â”‚ ğŸ§  Assistant [thinking...]                              â”‚
â”‚ I see the issue. The dependency resolution doesn't      â”‚
â”‚ account for circular dependencies. I'll need to modify  â”‚
â”‚ the topological sort algorithm...                        â”‚
â”‚                                                           â”‚
â”‚ [Scroll: j/k  Search: /  Copy: v  Export: E]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation**:
- Extend `EvaluationProgress` to include:
  ```rust
  pub conversation: Vec<ConversationMessage>,
  pub current_thinking: Option<String>,
  pub tool_calls: Vec<ToolCallDetail>,
  ```
- Agent loop sends message after each step
- TUI renders conversation in real-time

### P0.2: **Problem/Solution Context Panel**
**What**: Always show what the task is asking for

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“‹ Task Context                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ID: django__django-12345                                 â”‚
â”‚ Complexity: Medium                                       â”‚
â”‚                                                           â”‚
â”‚ ğŸ“ PROBLEM:                                              â”‚
â”‚ Migration dependencies are not correctly resolved when   â”‚
â”‚ there are circular references between apps. The system   â”‚
â”‚ should detect and break cycles gracefully.               â”‚
â”‚                                                           â”‚
â”‚ ğŸ¯ EXPECTED BEHAVIOR:                                    â”‚
â”‚ Migrations should run in correct order, breaking cycles  â”‚
â”‚ by introducing intermediate steps.                        â”‚
â”‚                                                           â”‚
â”‚ âœ… SUCCESS CRITERIA:                                     â”‚
â”‚ All tests in tests/migrations/test_dependencies.py pass â”‚
â”‚                                                           â”‚
â”‚ ğŸ“ FILES TO MODIFY:                                      â”‚
â”‚ â€¢ src/django/core/management/commands/migrate.py        â”‚
â”‚ â€¢ src/django/db/migrations/graph.py                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation**:
- Add `problem_context: Task` to `EvaluationProgress`
- Right sidebar shows task details
- Updates when task changes

### P0.3: **Step-by-Step Tool Execution Log**
**What**: Show every tool call with inputs/outputs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”§ Tool Execution Log                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Step  Tool      File/Command              Status  Time  â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚  1    Read      migrate.py                âœ…     120ms â”‚
â”‚  2    Read      graph.py                  âœ…      80ms â”‚
â”‚  3    Grep      "topological.*sort"       âœ…      45ms â”‚
â”‚  4    Edit      graph.py (+15, -3)        âœ…     200ms â”‚
â”‚  5    Bash      python manage.py test     âŒ    3.2s   â”‚
â”‚       â””â”€ Error: AssertionError: cycle not detected     â”‚
â”‚  6    Edit      graph.py (+2, -1)         âœ…     150ms â”‚
â”‚  7    Bash      python manage.py test     âœ…    3.1s   â”‚
â”‚       â””â”€ All tests passed (5/5)                         â”‚
â”‚                                                           â”‚
â”‚ [Click to expand tool output]                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation**:
- Capture tool inputs/outputs in agent loop
- Send via `EvaluationProgress.tool_calls`
- Render as expandable list in TUI

### P0.4: **Task Results with Full Details**
**What**: After task completes, show comprehensive results

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… Task Result: django__django-12345                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Status: SOLVED                                           â”‚
â”‚ Time: 127s    Cost: $0.145    Steps: 7/25    API: 12   â”‚
â”‚                                                           â”‚
â”‚ ğŸ“Š METRICS:                                              â”‚
â”‚ â€¢ Tests Passed: 5/5 (100%)                              â”‚
â”‚ â€¢ Files Modified: 2                                      â”‚
â”‚ â€¢ Lines Added: 17    Lines Removed: 4                   â”‚
â”‚ â€¢ Tool Calls: Read(3), Edit(2), Bash(2)                â”‚
â”‚                                                           â”‚
â”‚ âœ… SOLUTION:                                             â”‚
â”‚ Added cycle detection in topological_sort() and         â”‚
â”‚ introduced intermediate migration steps to break cycles â”‚
â”‚                                                           â”‚
â”‚ ğŸ“ FILES CHANGED:                                        â”‚
â”‚ â€¢ src/django/db/migrations/graph.py                     â”‚
â”‚   +17 -4  (added detect_cycles method)                  â”‚
â”‚ â€¢ src/django/core/management/commands/migrate.py        â”‚
â”‚   +0 -0   (no changes, read only)                       â”‚
â”‚                                                           â”‚
â”‚ [V]iew Diff  [R]eplay Execution  [E]xport              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### P0.5: **Persistent Results Storage**
**What**: Save detailed results to database/JSON for later viewing

**Storage Schema**:
```rust
pub struct DetailedEvalResult {
    pub run_id: String,                    // UUID for this eval run
    pub timestamp: DateTime<Utc>,
    pub milestone: u8,
    pub dataset: String,                   // "swebench-verified"
    pub task_count: usize,

    pub tasks: Vec<DetailedTaskResult>,

    pub summary: EvaluationResults,        // Existing summary stats
}

pub struct DetailedTaskResult {
    pub task: Task,                        // Full problem statement
    pub result: TaskResult,                // Outcome (solved/failed)

    pub conversation: Vec<Message>,        // Full LLM conversation
    pub tool_calls: Vec<ToolExecution>,   // All tool calls with I/O
    pub files_changed: Vec<FileDiff>,     // Actual diffs
    pub test_output: String,               // Test execution output
    pub error_log: Option<String>,         // Errors if failed
}
```

**Implementation**:
- Save to `./results/{run_id}/detailed.json`
- Index of runs in `./results/index.json`
- Load on startup for historical view

### P0.6: **Evaluation History Browser**
**What**: Browse past evaluation runs without starting new ones

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“š Evaluation History                  [Press H to open]â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Date/Time          Milestone  Tasks  Accuracy  Cost     â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚ 2025-11-11 10:30   M1         10     57.3%     $1.45   â”‚
â”‚ 2025-11-11 09:15   M2         10     63.1%     $2.18   â”‚
â”‚ 2025-11-10 14:22   M1         50     55.8%     $7.20   â”‚
â”‚ 2025-11-10 11:05   M1         10     60.0%     $1.52   â”‚
â”‚ 2025-11-09 16:40   M1         10     56.7%     $1.38   â”‚
â”‚                                                           â”‚
â”‚ [â†‘/â†“] Navigate  [Enter] View  [D]elete  [C]ompare       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Drill-Down**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“Š Run Details - 2025-11-11 10:30                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Milestone: M1    Dataset: SWE-bench Verified            â”‚
â”‚ Accuracy: 57.3%  Solved: 57/100  Cost: $14.52          â”‚
â”‚                                                           â”‚
â”‚ TASKS BY STATUS:                                         â”‚
â”‚ âœ… Solved (57)          [Click to view]                 â”‚
â”‚ âŒ Failed (43)          [Click to view]                 â”‚
â”‚                                                           â”‚
â”‚ TASK BREAKDOWN:                                          â”‚
â”‚ ID                      Status   Time    Cost    Steps  â”‚
â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚ django__django-12345    âœ…       127s    $0.15   7/25  â”‚
â”‚ requests__requests-789  âŒ       245s    $0.28  25/25  â”‚
â”‚ flask__flask-456        âœ…        89s    $0.12   5/25  â”‚
â”‚ ...                                                      â”‚
â”‚                                                           â”‚
â”‚ [Enter] Drill down  [R]eplay  [E]xport  [Back]         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ P1 FEATURES (Important - Better UX)

### P1.1: **Side-by-Side Comparison View**
**What**: Compare two evaluation runs side-by-side

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ M1 Baseline (2025-11-10)      â”‚ M2 AST Context (2025-11-11)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Accuracy: 57.3%               â”‚ Accuracy: 63.1%   (+5.8pp) âœ…â”‚
â”‚ Avg Cost: $0.12/task          â”‚ Avg Cost: $0.18   (+50%)   âš â”‚
â”‚ Avg Time: 134s                â”‚ Avg Time: 189s    (+41%)   âš â”‚
â”‚ Tasks Solved: 57/100          â”‚ Tasks Solved: 63/100       âœ…â”‚
â”‚                               â”‚                                â”‚
â”‚ ONLY M1 SOLVED (8 tasks):    â”‚ ONLY M2 SOLVED (14 tasks):    â”‚
â”‚ â€¢ django-11111                â”‚ â€¢ flask-22222                 â”‚
â”‚ â€¢ requests-33333              â”‚ â€¢ numpy-44444                 â”‚
â”‚ ...                           â”‚ ...                            â”‚
â”‚                               â”‚                                â”‚
â”‚ BOTH SOLVED (49 tasks)       â”‚ NEITHER SOLVED (29 tasks)     â”‚
â”‚                               â”‚                                â”‚
â”‚ [D]iff Tasks  [S]tatistics  [E]xport                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### P1.2: **Live Diff View for File Changes**
**What**: Show actual code changes as agent makes them

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“ Live Diff: src/django/db/migrations/graph.py         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 123  class MigrationGraph:                               â”‚
â”‚ 124      def __init__(self):                             â”‚
â”‚ 125          self.nodes = {}                             â”‚
â”‚ 126 +        self.cycle_detector = CycleDetector()   NEWâ”‚
â”‚ 127                                                       â”‚
â”‚ 128      def topological_sort(self):                     â”‚
â”‚ 129 -        return naive_sort(self.nodes)          OLD â”‚
â”‚ 130 +        # Detect cycles before sorting          NEWâ”‚
â”‚ 131 +        if self.cycle_detector.has_cycle():     NEWâ”‚
â”‚ 132 +            return self.break_cycles()          NEWâ”‚
â”‚ 133 +        return smart_sort(self.nodes)           NEWâ”‚
â”‚ 134                                                       â”‚
â”‚ [A]ccept  [R]eject  [V]iew Full  [E]xplain              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### P1.3: **Task Replay Mode**
**What**: Replay a past task execution step-by-step

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ¬ Replay: django__django-12345                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [â—€â—€] [â—€] [â–¶] [â–¶â–¶]  Step 3/7                           â”‚
â”‚                                                           â”‚
â”‚ Current Step: Edit graph.py                              â”‚
â”‚                                                           â”‚
â”‚ ğŸ¤– Agent Thought:                                        â”‚
â”‚ "I need to add cycle detection logic here..."           â”‚
â”‚                                                           â”‚
â”‚ ğŸ”§ Tool Call:                                            â”‚
â”‚ Edit(file="graph.py", diff="+15 -3")                    â”‚
â”‚                                                           â”‚
â”‚ ğŸ“„ Changes:                                              â”‚
â”‚ [Shows diff from previous step]                          â”‚
â”‚                                                           â”‚
â”‚ [Space] Play/Pause  [â†/â†’] Prev/Next  [0] Restart       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### P1.4: **Failed Task Analysis**
**What**: Dedicated view for understanding failures

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âŒ Failure Analysis: requests__requests-789              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ WHY IT FAILED:                                           â”‚
â”‚ â€¢ Agent hit max steps (25/25) without solution          â”‚
â”‚ â€¢ Tests still failing: 2/5 passed                       â”‚
â”‚ â€¢ Error: ConnectionTimeout in test_retry_logic          â”‚
â”‚                                                           â”‚
â”‚ WHAT AGENT TRIED:                                        â”‚
â”‚ Step 1-5:   Read relevant files                         â”‚
â”‚ Step 6-12:  Made edits to retry logic                   â”‚
â”‚ Step 13-15: Ran tests, got failures                     â”‚
â”‚ Step 16-20: Tried to fix timeout handling               â”‚
â”‚ Step 21-25: Still failing, gave up                      â”‚
â”‚                                                           â”‚
â”‚ LIKELY ISSUE:                                            â”‚
â”‚ Agent didn't understand async timeout semantics         â”‚
â”‚                                                           â”‚
â”‚ SUGGESTIONS:                                             â”‚
â”‚ â€¢ May need better async code understanding (M2 feature) â”‚
â”‚ â€¢ Should add timeout hints to problem statement         â”‚
â”‚                                                           â”‚
â”‚ [R]eplay  [V]iew Conversation  [C]ompare with Similar   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### P1.5: **Statistical Dashboard**
**What**: Visual charts and graphs for trends

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“Š Statistical Dashboard                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ACCURACY TREND:                                          â”‚
â”‚ 70% â”‚                                              â–„     â”‚
â”‚ 60% â”‚                                   â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆ     â”‚
â”‚ 50% â”‚                       â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆ                 â”‚
â”‚ 40% â”‚           â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆ                             â”‚
â”‚ 30% â”‚   â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆ                                        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚
â”‚      M0   M1(v1) M1(v2) M1(v3)  M2(v1)  M2(v2)  M3    â”‚
â”‚                                                           â”‚
â”‚ COST vs ACCURACY:                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚ A â”‚              M3 â—                    â”‚ Best balance â”‚
â”‚ c â”‚           M2 â—                       â”‚              â”‚
â”‚ c â”‚        M1 â—                          â”‚              â”‚
â”‚ u â”‚     M0 â—                             â”‚              â”‚
â”‚ r â”‚                                      â”‚              â”‚
â”‚ a â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚              â”‚
â”‚ c              Cost per Task             â”‚              â”‚
â”‚ y                                        â”‚              â”‚
â”‚                                                           â”‚
â”‚ [V]iew Runs  [F]ilter  [E]xport                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### P1.6: **Quick Re-Run Failed Tasks**
**What**: Re-run specific failed tasks from history

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”„ Re-Run Failed Tasks                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ From: M1 Baseline (2025-11-11 10:30)                    â”‚
â”‚                                                           â”‚
â”‚ Select tasks to re-run:                                  â”‚
â”‚ â˜‘ requests__requests-789     (Failed: timeout)          â”‚
â”‚ â˜‘ flask__flask-456           (Failed: max steps)        â”‚
â”‚ â˜ numpy__numpy-123           (Failed: test error)       â”‚
â”‚ â˜‘ pandas__pandas-999         (Failed: syntax error)     â”‚
â”‚                                                           â”‚
â”‚ Re-run with:                                             â”‚
â”‚ â€¢ Milestone: M2  [Change]                               â”‚
â”‚ â€¢ Max steps: 30  [Change]                               â”‚
â”‚ â€¢ Timeout: 300s  [Change]                               â”‚
â”‚                                                           â”‚
â”‚ [A]ll Failed  [N]one  [R]un Selected (3 tasks)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¨ P2 FEATURES (Nice to Have - Polish)

### P2.1: **Export to Multiple Formats**
- JSON (detailed results)
- CSV (for spreadsheet analysis)
- HTML (shareable report)
- Markdown (for GitHub/docs)

### P2.2: **Search/Filter Results**
```
Search: [django____________]  [Go]

Filters:
â˜‘ Status: Solved  â˜ Failed
â˜‘ Complexity: Medium  â˜‘ Hard
â˜ Duration > 120s
â˜ Cost > $0.20
```

### P2.3: **Annotations/Notes**
Add notes to specific runs or tasks:
```
ğŸ“ Notes for django__django-12345:
"This task requires understanding of Django's migration
 graph data structure. Good test case for AST context."
```

### P2.4: **Tag/Label System**
Tag runs for organization:
```
Tags: #baseline #m1 #verified-dataset #production-ready
```

### P2.5: **Automated Insights**
AI-generated insights:
```
ğŸ’¡ Insights:
â€¢ M2 performs 8.5% better on "Hard" complexity tasks
â€¢ Average cost increased 42% but accuracy gain is 5.8pp
â€¢ ROI: $0.13 per percentage point improvement
â€¢ Recommendation: ADOPT M2 for production
```

### P2.6: **Live Collaboration**
Share live evaluation links:
```
Share: https://toad.dev/eval/abc123
Others can watch your eval run in real-time
```

---

## ğŸ—ï¸ IMPLEMENTATION PLAN

### Phase 1: Real-Time Visibility (P0.1-P0.3)
**Goal**: See what's happening during eval
**Time**: 2-3 days

1. Extend `EvaluationProgress` struct
2. Modify agent loop to capture conversation
3. Capture tool call details (inputs/outputs)
4. Update TUI to render conversation panel
5. Add tool execution log widget

### Phase 2: Persistent Storage (P0.4-P0.6)
**Goal**: Save and browse historical results
**Time**: 2-3 days

1. Design `DetailedEvalResult` schema
2. Save to JSON after each run
3. Create index file for fast lookup
4. Build history browser UI
5. Add drill-down into specific runs

### Phase 3: Comparison Tools (P1.1-P1.2)
**Goal**: Compare runs side-by-side
**Time**: 2 days

1. Load two runs simultaneously
2. Build side-by-side comparison view
3. Add diff highlighting
4. Show which tasks differ

### Phase 4: Replay & Analysis (P1.3-P1.5)
**Goal**: Understand failures better
**Time**: 3 days

1. Build task replay engine
2. Create failure analysis view
3. Add statistical dashboard
4. Implement trend charts

### Phase 5: Polish (P2.1-P2.6)
**Goal**: Professional-grade UX
**Time**: 2-3 days

1. Export formats
2. Search/filter
3. Notes/tags
4. Automated insights

---

## ğŸ“ PROPOSED UI LAYOUT (Evaluation Screen)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ¸ TOAD  Eval: M1 Baseline  ğŸ”„ Running  Task 3/10  $0.42    â”‚ Status
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ“‹ Task      â”‚ ğŸ¤– Agent Conversation        â”‚ ğŸ”§ Tools       â”‚
â”‚ â”â”â”â”â”â”â”â”     â”‚ â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”  â”‚ â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚ ID: django-  â”‚ Step 5/25  Tokens: 2.3k      â”‚ Read     x3    â”‚
â”‚ 12345        â”‚                               â”‚ Edit     x1    â”‚
â”‚              â”‚ ğŸ§  Assistant [thinking...]   â”‚ Bash     x1    â”‚
â”‚ PROBLEM:     â”‚ I need to add cycle          â”‚ Grep     x0    â”‚
â”‚ Migration    â”‚ detection here...             â”‚                â”‚
â”‚ dependencies â”‚                               â”‚ CURRENT:       â”‚
â”‚ not resolved â”‚ ğŸ”§ Tool: Edit graph.py       â”‚ Edit           â”‚
â”‚ correctly... â”‚                               â”‚ graph.py       â”‚
â”‚              â”‚ ğŸ“„ Output:                    â”‚ +15 -3         â”‚
â”‚ FILES:       â”‚ ```python                     â”‚                â”‚
â”‚ â˜‘ migrate.py â”‚ +def detect_cycles():         â”‚ STATUS:        â”‚
â”‚ â˜‘ graph.py   â”‚   ...                         â”‚ In progress... â”‚
â”‚              â”‚ ```                            â”‚                â”‚
â”‚ TESTS:       â”‚                               â”‚ â”â”â”â”â”â”â”â”â”â”â”â”  â”‚
â”‚ 0/5 passed   â”‚ [j/k scroll  / search]       â”‚ Step  Tool     â”‚
â”‚              â”‚                               â”‚  1    Read     â”‚
â”‚ [V]iew Full  â”‚                               â”‚  2    Read     â”‚
â”‚              â”‚                               â”‚  3    Grep     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Progress: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 60%  Est: 45s remain â”‚ Progress
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ctrl+C:Cancel  H:History  R:Retry  S:Save  E:Export  Q:Quit â”‚ Shortcuts
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ SUCCESS METRICS

After implementing P0-P2:

### Visibility
- âœ… Can see every agent step in real-time
- âœ… Can see tool calls with full I/O
- âœ… Can see problem + expected solution
- âœ… Can see why tasks fail

### Historical Analysis
- âœ… Can browse all past runs
- âœ… Can compare any two runs
- âœ… Can replay any task
- âœ… Can export detailed reports

### Developer Productivity
- â± Time to debug failed task: < 2 minutes (vs âˆ now)
- â± Time to compare two runs: < 30 seconds
- â± Time to find regression: < 1 minute
- ğŸ“Š Confidence in eval results: 100% (vs ~50% now)

### User Experience
- ğŸ¯ Evaluation feels "alive" with real-time updates
- ğŸ” Can investigate any result thoroughly
- ğŸ“ˆ Can track progress over time
- ğŸš€ Testing center is professional-grade tool

---

## ğŸ“ IMMEDIATE NEXT STEPS

1. **Extend EvaluationProgress** (30 min)
   - Add conversation, tool_calls, current_task fields

2. **Capture Agent Conversation** (2 hours)
   - Modify agent loop to save messages
   - Send via progress updates

3. **Build Real-Time Conversation View** (3 hours)
   - Create scrollable conversation widget
   - Render messages with syntax highlighting

4. **Test with Simple Eval** (1 hour)
   - Run `eval --count 1` and verify visibility

**Total for MVP visibility**: ~6 hours of focused work

---

**Ready to implement?** Start with Phase 1 P0.1-P0.3 for immediate impact!
